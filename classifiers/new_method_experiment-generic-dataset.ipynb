{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install openai sqlitedict seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "# MODEL = \"babbage-002\"\n",
    "MODEL = \"davinci-002\"\n",
    "# MODEL = \"df@meta-llama/Llama-2-70b-chat-hf\"\n",
    "# MODEL = \"df@databricks/dbrx-instruct\"\n",
    "# MODEL = \"lambda_1x@TheBloke/Llama-2-70B-AWQ\"\n",
    "\n",
    "# MODEL = \"df@meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# MODEL = \"lambda@meta-llama/Meta-Llama-3-70B\"\n",
    "# MODEL = \"lambda@meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# MODEL = \"lambda@TheBloke/mixtral-8x7b-v0.1-AWQ\"\n",
    "\n",
    "# Conditioners\n",
    "CONDITIONERS = 2\n",
    "\n",
    "# Sample rate parameters\n",
    "SAMPLE_SIZE = -1\n",
    "SAMPLE_COUNT = -1\n",
    "SAMPLE_MULTIPLIER = 1\n",
    "\n",
    "# Corpus\n",
    "CORPUS_NAME = \"prison_reform\"\n",
    "\n",
    "# Mode - either seeds or sentences\n",
    "# MODE = \"seeds\"\n",
    "MODE = \"sentences\"\n",
    "\n",
    "# The name of the experiment (i.e. where to save the results)\n",
    "EXPERIMENT_NAME = (\n",
    "    (\n",
    "        f\"{MODEL}_{CORPUS_NAME}_mode{MODE}_cond{CONDITIONERS}_samp{SAMPLE_SIZE}_mult{SAMPLE_MULTIPLIER}\"\n",
    "    )\n",
    "    .replace(\"@\", \"_\")\n",
    "    .replace(\"/\", \"_\")\n",
    ")\n",
    "\n",
    "# Whether we're in debug mode\n",
    "DEBUG = False\n",
    "\n",
    "# Corpus Size\n",
    "CORPUS_SIZE = \"/16_8\"\n",
    "\n",
    "OUTPUT_DIR = \"corpus_results/generic\"\n",
    "\n",
    "print(f\"*** Beginning Experiment: '{EXPERIMENT_NAME}' ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write config to config.json\n",
    "import json\n",
    "\n",
    "config = {\n",
    "    \"MODEL\": MODEL,\n",
    "    \"CONDITIONERS\": CONDITIONERS,\n",
    "    \"SAMPLE_SIZE\": SAMPLE_SIZE,\n",
    "    \"SAMPLE_COUNT\": SAMPLE_COUNT,\n",
    "    \"SAMPLE_MULTIPLIER\": SAMPLE_MULTIPLIER,\n",
    "    \"CORPUS_NAME\": CORPUS_NAME,\n",
    "    \"MODE\": MODE,\n",
    "    \"EXPERIMENT_NAME\": EXPERIMENT_NAME,\n",
    "    \"DEBUG\": DEBUG,\n",
    "    \"CORPUS_SIZE\": CORPUS_SIZE,\n",
    "    \"OUTPUT_DIR\": OUTPUT_DIR,\n",
    "}\n",
    "\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from analyze import experiment_set, NarrativeContext\n",
    "\n",
    "ctx = NarrativeContext(\n",
    "    model=MODEL,\n",
    "    conditioners=CONDITIONERS,\n",
    "    # sample_rate=SAMPLE_SIZE\n",
    ")\n",
    "\n",
    "# Create the results directory\n",
    "RESULTS_DIR = os.path.join(OUTPUT_DIR, EXPERIMENT_NAME)\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.makedirs(RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiprocessing with joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "import pickle\n",
    "from analyze import (\n",
    "    join_many_strings,\n",
    "    get_logits,\n",
    "    likelihood_delta,\n",
    "    stratified_n_tuple_sampling,\n",
    ")\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "\n",
    "\n",
    "ROOT_DIR = \"/workspaces/dev/projects/narratives/synthetic/corpora\"\n",
    "SUBFOLDER_DIR = \"seed_gpt-4o_sentence_gpt-4o_temp_0.0\"\n",
    "\n",
    "\n",
    "def load_dataset(generic_name):\n",
    "    with open(\n",
    "        os.path.join(ROOT_DIR, generic_name, SUBFOLDER_DIR, \"N_16_K_8.json\"), \"r\"\n",
    "    ) as f:\n",
    "        print(\"Loading dataset\")\n",
    "        data = yaml.safe_load(f)\n",
    "        print(\"Loaded dataset\")\n",
    "\n",
    "    print(data)\n",
    "    # raise ValueError(\"Done\")\n",
    "\n",
    "    seeds = data[\"seeds\"]\n",
    "    distilled = data[\"distilled\"]\n",
    "    summarized = data[\"summarized\"]\n",
    "    names = data[\"names\"]\n",
    "    dataset = data[\"dataset\"]\n",
    "\n",
    "    if MODE == \"seeds\":\n",
    "        X, y = seeds_mode(seeds)\n",
    "    elif MODE == \"sentences\":\n",
    "        X = []\n",
    "        y = []\n",
    "        for datum in dataset:\n",
    "            a_first = datum[\"a_first\"]\n",
    "            b_first = datum[\"b_first\"]\n",
    "\n",
    "            # Add a sentences\n",
    "            X += a_first[\"a\"]\n",
    "            y += [\"a\"] * len(a_first[\"a\"])\n",
    "            X += b_first[\"a\"]\n",
    "            y += [\"a\"] * len(b_first[\"a\"])\n",
    "\n",
    "            # Add b sentences\n",
    "            X += a_first[\"b\"]\n",
    "            y += [\"b\"] * len(a_first[\"b\"])\n",
    "            X += b_first[\"b\"]\n",
    "            y += [\"b\"] * len(b_first[\"b\"])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode: {}\".format(MODE))\n",
    "\n",
    "    return X, y, seeds, distilled, summarized, names\n",
    "\n",
    "\n",
    "def seeds_mode(seeds):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for seed_def in seeds:\n",
    "        a = seed_def[\"a\"]\n",
    "        b = seed_def[\"b\"]\n",
    "        X.append(a)\n",
    "        y.append(\"a\")\n",
    "        X.append(b)\n",
    "        y.append(\"b\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# def get_seeds(path):\n",
    "#     with open(path, \"r\") as f:\n",
    "#         corpus_data = yaml.safe_load(f)\n",
    "\n",
    "#     seeds = corpus_data[\"seeds\"]\n",
    "#     distilled = corpus_data[\"distilled\"]\n",
    "#     summarized = corpus_data[\"summarized\"]\n",
    "#     names = corpus_data[\"names\"]\n",
    "#     dataset = corpus_data[\"dataset\"]\n",
    "\n",
    "#     if DATASET_TYPE == \"distilled\":\n",
    "#         a_s = []\n",
    "#         b_s = []\n",
    "#         for seed_pair in distilled:\n",
    "#             a_s.append(seed_pair[\"a\"])\n",
    "#             b_s.append(seed_pair[\"b\"])\n",
    "#     elif DATASET_TYPE == \"summaries\":\n",
    "#         a_s = [summarized[\"a\"]]\n",
    "#         b_s = [summarized[\"b\"]]\n",
    "#     else:\n",
    "#         raise ValueError(\"Invalid dataset type: {}\".format(DATASET_TYPE))\n",
    "\n",
    "#     print(\"a\", len(a_s))\n",
    "#     print(\"b\", len(b_s))\n",
    "#     return a_s, b_s, []\n",
    "\n",
    "\n",
    "# def get_sentences(path):\n",
    "#     with open(path, \"r\") as f:\n",
    "#         corpus_data = yaml.safe_load(f)\n",
    "\n",
    "#     seeds = corpus_data[\"seeds\"]\n",
    "#     distilled = corpus_data[\"distilled\"]\n",
    "#     summarized = corpus_data[\"summarized\"]\n",
    "#     names = corpus_data[\"names\"]\n",
    "#     dataset = corpus_data[\"dataset\"]\n",
    "\n",
    "#     a_s = []\n",
    "#     b_s = []\n",
    "#     for datum in dataset:\n",
    "#         a_first = datum[\"a_first\"]\n",
    "#         b_first = datum[\"b_first\"]\n",
    "\n",
    "#         # Add a sentences\n",
    "#         a_s += a_first[\"a\"]\n",
    "#         a_s += b_first[\"a\"]\n",
    "\n",
    "#         # Add b sentences\n",
    "#         b_s += a_first[\"b\"]\n",
    "#         b_s += b_first[\"b\"]\n",
    "\n",
    "#     print(\"a\", len(a_s))\n",
    "#     print(\"b\", len(b_s))\n",
    "\n",
    "#     # Shuffle a_s and b_s\n",
    "#     random.shuffle(a_s)\n",
    "#     random.shuffle(b_s)\n",
    "\n",
    "#     # Return\n",
    "#     return a_s, b_s, []\n",
    "\n",
    "\n",
    "def align_text_to_conditioners(label, narrative_sets, text, ctx, sample_size):\n",
    "    # No batching\n",
    "    deltas = []\n",
    "    for conditioners in narrative_sets:\n",
    "        conditioner_string = join_many_strings(conditioners)\n",
    "        deltas.append(\n",
    "            {\n",
    "                \"conditioners\": conditioners,\n",
    "                \"likelihood_delta\": likelihood_delta(\n",
    "                    conditioner_string, text[\"text\"], ctx\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return {\"text\": text, \"likelihood_deltas\": deltas}\n",
    "\n",
    "\n",
    "def align(label, narrative_set, texts, sample_size):\n",
    "    print(\"*** Aligning {} texts to {} ***\".format(len(texts), label))\n",
    "    results = []\n",
    "    for text in tqdm(texts):\n",
    "        results.append(\n",
    "            align_text_to_conditioners(\n",
    "                label,\n",
    "                pairs[(tuple(narrative_set), text[\"text\"])],\n",
    "                text,\n",
    "                ctx,\n",
    "                sample_size,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Load texts from corpora/{CORPUS}.json\n",
    "# with open(os.path.join(\"corpora\", f\"{CORPUS}.json\"), \"r\") as f:\n",
    "#     corpus_data = yaml.safe_load(f)\n",
    "from import_data import import_data\n",
    "\n",
    "X, y, seeds, distilled, summarized, names = load_dataset(CORPUS_NAME)\n",
    "\n",
    "# Strip surrounding \" from outside of text in X\n",
    "X = [text.strip('\"') for text in X]\n",
    "\n",
    "# Get seeds\n",
    "a_s = [seed[\"a\"] for seed in seeds]\n",
    "b_s = [seed[\"b\"] for seed in seeds]\n",
    "neutral = []\n",
    "\n",
    "a_sentences = []\n",
    "b_sentences = []\n",
    "for _x, _y in zip(X, y):\n",
    "    if _y == \"a\":\n",
    "        a_sentences.append(_x)\n",
    "    elif _y == \"b\":\n",
    "        b_sentences.append(_x)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label: {}\".format(_y))\n",
    "\n",
    "sentences = a_sentences + b_sentences\n",
    "random.shuffle(sentences)\n",
    "corpus_data = [\n",
    "    {\"text\": text, \"speakername\": label, \"date\": \"2024-03-06\"}\n",
    "    for text, label in zip(sentences, y)\n",
    "]\n",
    "\n",
    "print(\"Loaded {} texts from corpus\".format(len(corpus_data)))\n",
    "print(\"Total word count:\", sum([len(text[\"text\"].split()) for text in corpus_data]))\n",
    "\n",
    "# Use SAMPLE_COUNT\n",
    "if SAMPLE_COUNT != -1 and len(corpus_data) > SAMPLE_COUNT:\n",
    "    corpus_data = random.sample(corpus_data, SAMPLE_COUNT)\n",
    "\n",
    "\n",
    "# Precompute narrative set -- text pairs\n",
    "pairs = {}\n",
    "to_precompute = set()\n",
    "for narrative_set in [a_s, b_s, neutral]:\n",
    "    for text in corpus_data:\n",
    "        # # Pick a random narrative set\n",
    "        # narrative_sets = list(itertools.permutations(narrative_set, CONDITIONERS))\n",
    "        # narrative_sets = random.sample(narrative_sets, min(len(narrative_sets), len(narrative_set)))\n",
    "        narrative_sets = []\n",
    "        for _ in range(SAMPLE_MULTIPLIER):\n",
    "            narrative_sets_iter = stratified_n_tuple_sampling(\n",
    "                list(narrative_set), CONDITIONERS\n",
    "            )\n",
    "            narrative_sets.extend(narrative_sets_iter)\n",
    "\n",
    "        # Append to the list\n",
    "        pairs[(tuple(narrative_set), text[\"text\"])] = narrative_sets\n",
    "\n",
    "        # Add to the set of things to precompute\n",
    "        for conditioners in narrative_sets:\n",
    "            conditioner_string = join_many_strings(conditioners)\n",
    "            to_precompute.add(join_many_strings([conditioner_string, text[\"text\"]]))\n",
    "            to_precompute.add(conditioner_string)\n",
    "            to_precompute.add(text[\"text\"])\n",
    "\n",
    "# Get the total word count of the precompute set\n",
    "print(\n",
    "    \"Total word count of precompute set:\",\n",
    "    sum([len(text.split()) for text in to_precompute]),\n",
    ")\n",
    "# break\n",
    "\n",
    "# Precompute the logits\n",
    "print(\"--- Precomputing logits ---\")\n",
    "logits, usages = get_logits(list(to_precompute), ctx.model, num_threads=8)\n",
    "print(usages[0])\n",
    "print(\"--- Done precomputing logits ---\")\n",
    "\n",
    "# Run experiments\n",
    "results = {\n",
    "    \"a\": align(\"a\", a_s, corpus_data, None),\n",
    "    \"b\": align(\"b\", b_s, corpus_data, None),\n",
    "}\n",
    "\n",
    "# Save results with pickle\n",
    "with open(\n",
    "    os.path.join(\n",
    "        OUTPUT_DIR,\n",
    "        EXPERIMENT_NAME,\n",
    "        # f\"{CORPUS}_{MODEL}_{CONDITIONERS}_{SAMPLE_MULTIPLIER}.pkl\",\n",
    "        \"results.pkl\",\n",
    "    ),\n",
    "    \"wb\",\n",
    ") as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save usage data to CSV\n",
    "# Usage object is like\n",
    "# CompletionUsage(completion_tokens=1, prompt_tokens=43, total_tokens=44)\n",
    "# We want to save this as two columns - prompt_tokens and completion_tokens\n",
    "import pandas as pd\n",
    "\n",
    "usage_data = []\n",
    "for usage in usages:\n",
    "    usage_data.append(\n",
    "        {\n",
    "            \"prompt_tokens\": usage.prompt_tokens,\n",
    "            \"completion_tokens\": usage.completion_tokens,\n",
    "        }\n",
    "    )\n",
    "\n",
    "usage_df = pd.DataFrame(usage_data)\n",
    "usage_df.to_csv(\n",
    "    os.path.join(\n",
    "        OUTPUT_DIR,\n",
    "        EXPERIMENT_NAME,\n",
    "        # f\"{CORPUS}_{MODEL}_{CONDITIONERS}_{SAMPLE_MULTIPLIER}_usage.csv\",\n",
    "        \"usage.csv\",\n",
    "    ),\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "\n",
    "# Load data from pkl\n",
    "with open(\n",
    "    os.path.join(\n",
    "        OUTPUT_DIR,\n",
    "        EXPERIMENT_NAME,\n",
    "        # f\"{CORPUS}_{MODEL}_{CONDITIONERS}_{SAMPLE_MULTIPLIER}.pkl\",\n",
    "        \"results.pkl\",\n",
    "    ),\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# For each narrative set in (helpful, unhelpful), find the texts with the highest and lowest likelihood delta (relative to the other set\n",
    "# of narratives) and print them.\n",
    "a_s = results[\"a\"]\n",
    "b_s = results[\"b\"]\n",
    "\n",
    "\n",
    "def compute_deltas(result_set):\n",
    "    # Compute generally for helpful/unhelpful/neutral\n",
    "    deltas = {}\n",
    "    for text in result_set:\n",
    "        text_deltas = [\n",
    "            delta[\"likelihood_delta\"][\"likelihood_delta\"]\n",
    "            for delta in text[\"likelihood_deltas\"]\n",
    "        ]\n",
    "        deltas[text[\"text\"][\"text\"]] = np.mean(text_deltas)\n",
    "        # Get the signed max abs value\n",
    "        # deltas[text[\"text\"][\"text\"]] = max(text_deltas, key=abs)\n",
    "        # deltas[text[\"text\"][\"text\"]] = max(text_deltas)\n",
    "        print(text)\n",
    "    return deltas\n",
    "\n",
    "\n",
    "helpful_deltas = compute_deltas(a_s)\n",
    "unhelpful_deltas = compute_deltas(b_s)\n",
    "\n",
    "helpful_unhelpful_diffs = []\n",
    "for text in a_s:\n",
    "    helpful_unhelpful_diffs.append(\n",
    "        {\n",
    "            \"text\": text[\"text\"][\"text\"],\n",
    "            \"a\": helpful_deltas[text[\"text\"][\"text\"]],\n",
    "            \"b\": unhelpful_deltas[text[\"text\"][\"text\"]],\n",
    "            # \"helpful_deltas\": [delta[\"likelihood_delta\"][\"likelihood_delta\"] for delta in text[\"likelihood_deltas\"]],\n",
    "            # \"unhelpful_deltas\": [delta[\"likelihood_delta\"][\"likelihood_delta\"] for delta in text[\"likelihood_deltas\"]],\n",
    "            \"speaker\": text[\"text\"][\"speakername\"],\n",
    "            \"affiliation\": (\n",
    "                text[\"text\"][\"affiliation\"] if \"affiliation\" in text[\"text\"] else None\n",
    "            ),\n",
    "            \"date\": text[\"text\"][\"date\"],\n",
    "            \"diff\": helpful_deltas[text[\"text\"][\"text\"]]\n",
    "            - unhelpful_deltas[text[\"text\"][\"text\"]],\n",
    "        }\n",
    "    )\n",
    "\n",
    "helpful_unhelpful_diffs = sorted(helpful_unhelpful_diffs, key=lambda x: x[\"diff\"])\n",
    "\n",
    "print(\"Most unhelpful:\")\n",
    "for text in helpful_unhelpful_diffs[:10]:\n",
    "    print(f\"[{text['diff']}] {text['text']}\")\n",
    "print()\n",
    "print(\"Most helpful:\")\n",
    "for text in helpful_unhelpful_diffs[-10:]:\n",
    "    print(f\"[{text['diff']}] {text['text']}\")\n",
    "print()\n",
    "\n",
    "outpath = os.path.join(\n",
    "    OUTPUT_DIR,\n",
    "    EXPERIMENT_NAME,\n",
    "    # f\"{CORPUS}_{MODEL}_{CONDITIONERS}_{SAMPLE_MULTIPLIER}\",\n",
    "    \"results\",\n",
    ")\n",
    "print(\"Saving to:\", outpath)\n",
    "\n",
    "# Save results with JSON\n",
    "with open(outpath + \".json\", \"w\") as f:\n",
    "    json.dump(helpful_unhelpful_diffs, f, indent=4)\n",
    "\n",
    "# Save in CSV format\n",
    "df = pd.DataFrame(helpful_unhelpful_diffs)\n",
    "df.to_csv(outpath + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
