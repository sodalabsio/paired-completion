{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddaef053",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [6]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff0445a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T11:16:52.106646Z",
     "iopub.status.busy": "2024-05-17T11:16:52.106357Z",
     "iopub.status.idle": "2024-05-17T11:16:52.109414Z",
     "shell.execute_reply": "2024-05-17T11:16:52.108914Z"
    },
    "papermill": {
     "duration": 0.0111,
     "end_time": "2024-05-17T11:16:52.110637",
     "exception": false,
     "start_time": "2024-05-17T11:16:52.099537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install openai sqlitedict seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04fec61c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T11:16:52.122488Z",
     "iopub.status.busy": "2024-05-17T11:16:52.122015Z",
     "iopub.status.idle": "2024-05-17T11:16:52.129851Z",
     "shell.execute_reply": "2024-05-17T11:16:52.129463Z"
    },
    "papermill": {
     "duration": 0.014781,
     "end_time": "2024-05-17T11:16:52.130992",
     "exception": false,
     "start_time": "2024-05-17T11:16:52.116211",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "# MODEL = \"babbage-002\"\n",
    "# MODEL = \"df@meta-llama/Llama-2-70b-chat-hf\"\n",
    "# MODEL = \"df@databricks/dbrx-instruct\"\n",
    "MODEL = \"lambda@TheBloke/Llama-2-70B-AWQ\"\n",
    "# MODEL = \"lambda@meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# MODEL = \"lambda@TheBloke/mixtral-8x7b-v0.1-AWQ\"\n",
    "\n",
    "# Conditioners\n",
    "CONDITIONERS = 2\n",
    "\n",
    "# Sample rate parameters\n",
    "SAMPLE_SIZE = -1\n",
    "SAMPLE_COUNT = -1\n",
    "SAMPLE_MULTIPLIER = 1\n",
    "\n",
    "# Corpus\n",
    "# CORPUS_NAME = \"climate_change\"\n",
    "# CORPUS_NAME = \"voice\"\n",
    "# CORPUS_NAME = \"climate\"\n",
    "CORPUS_NAME = \"asylum_seekers\"\n",
    "\n",
    "# The name of the experiment (i.e. where to save the results)\n",
    "EXPERIMENT_NAME = \"llama-2-70B-asylum-seekers\"\n",
    "\n",
    "# Whether we're in debug mode\n",
    "DEBUG = False\n",
    "\n",
    "# Corpus Size\n",
    "CORPUS_SIZE = \"/2_2\"\n",
    "\n",
    "# The dataset type\n",
    "DATASET_TYPE = \"distilled\"\n",
    "# DATASET_TYPE = \"summaries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e096648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T11:16:52.142996Z",
     "iopub.status.busy": "2024-05-17T11:16:52.142771Z",
     "iopub.status.idle": "2024-05-17T11:16:52.146053Z",
     "shell.execute_reply": "2024-05-17T11:16:52.145681Z"
    },
    "papermill": {
     "duration": 0.010279,
     "end_time": "2024-05-17T11:16:52.147184",
     "exception": false,
     "start_time": "2024-05-17T11:16:52.136905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CORPUS_NAME == \"voice\":\n",
    "    CORPUS = \"voice\"\n",
    "    # CORPUS = \"/workspaces/dev/projects/narratives/classifiers/real_world_corpora/the_voice_the_voice_broad_keyscheck_4sep2023_filtered_chars150to1200_gpt-3.5-turbo-instruct_2_2 (1).json\"\n",
    "elif CORPUS_NAME == \"climate\":\n",
    "    CORPUS = \"climate\"\n",
    "    # CORPUS = \"/workspaces/dev/projects/narratives/classifiers/real_world_corpora/with_jensen_garrett_abbott_climate_climate_change_pms_curie_2_-1.json\"\n",
    "elif CORPUS_NAME == \"asylum_seekers\":\n",
    "    CORPUS = \"asylum_seekers\"\n",
    "else:\n",
    "    CORPUS = (\n",
    "        \"/workspaces/dev/projects/narratives/synthetic/gpt-4-only-corpora/\"\n",
    "        + CORPUS_NAME\n",
    "        + CORPUS_SIZE\n",
    "        + \".json\"\n",
    "    )\n",
    "\n",
    "# Replace @ and / in the experiment name\n",
    "EXPERIMENT_NAME = EXPERIMENT_NAME.replace(\"@\", \"_\").replace(\"/\", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627d9458",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T11:16:52.158658Z",
     "iopub.status.busy": "2024-05-17T11:16:52.158383Z",
     "iopub.status.idle": "2024-05-17T11:16:52.160309Z",
     "shell.execute_reply": "2024-05-17T11:16:52.159951Z"
    },
    "papermill": {
     "duration": 0.008924,
     "end_time": "2024-05-17T11:16:52.161463",
     "exception": false,
     "start_time": "2024-05-17T11:16:52.152539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # pip install\n",
    "# %pip install openai dill seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a33ae40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T11:16:52.173106Z",
     "iopub.status.busy": "2024-05-17T11:16:52.172844Z",
     "iopub.status.idle": "2024-05-17T11:16:52.596842Z",
     "shell.execute_reply": "2024-05-17T11:16:52.596383Z"
    },
    "papermill": {
     "duration": 0.431128,
     "end_time": "2024-05-17T11:16:52.598016",
     "exception": false,
     "start_time": "2024-05-17T11:16:52.166888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from analyze import experiment_set, NarrativeContext\n",
    "\n",
    "ctx = NarrativeContext(\n",
    "    model=MODEL,\n",
    "    conditioners=CONDITIONERS,\n",
    "    # sample_rate=SAMPLE_SIZE\n",
    ")\n",
    "\n",
    "# Create the results directory\n",
    "RESULTS_DIR = os.path.join(\"corpus_results/diff_classification\", EXPERIMENT_NAME)\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.makedirs(RESULTS_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca7dce8",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e820d4de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T11:16:52.611245Z",
     "iopub.status.busy": "2024-05-17T11:16:52.611022Z",
     "iopub.status.idle": "2024-05-17T11:16:53.039784Z",
     "shell.execute_reply": "2024-05-17T11:16:53.039357Z"
    },
    "papermill": {
     "duration": 0.436339,
     "end_time": "2024-05-17T11:16:53.040792",
     "exception": true,
     "start_time": "2024-05-17T11:16:52.604453",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'asylum_seekers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1067564/2628790741.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mCORPUS_NAME\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"voice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"climate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"asylum_seekers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistilled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCORPUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mhelpful\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/dev/projects/narratives/classifiers/import_data.py\u001b[0m in \u001b[0;36mimport_data\u001b[0;34m(CORPUS)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCORPUS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m             \u001b[0mcorpus_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'asylum_seekers'"
     ]
    }
   ],
   "source": [
    "# Multiprocessing with joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "import pickle\n",
    "from analyze import (\n",
    "    join_many_strings,\n",
    "    get_logits,\n",
    "    likelihood_delta,\n",
    "    stratified_n_tuple_sampling,\n",
    ")\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "\n",
    "\n",
    "def get_seeds(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        corpus_data = yaml.safe_load(f)\n",
    "\n",
    "    seeds = corpus_data[\"seeds\"]\n",
    "    distilled = corpus_data[\"distilled\"]\n",
    "    summarized = corpus_data[\"summarized\"]\n",
    "    names = corpus_data[\"names\"]\n",
    "    dataset = corpus_data[\"dataset\"]\n",
    "\n",
    "    if DATASET_TYPE == \"distilled\":\n",
    "        a_s = []\n",
    "        b_s = []\n",
    "        for seed_pair in distilled:\n",
    "            a_s.append(seed_pair[\"a\"])\n",
    "            b_s.append(seed_pair[\"b\"])\n",
    "    elif DATASET_TYPE == \"summaries\":\n",
    "        a_s = [summarized[\"a\"]]\n",
    "        b_s = [summarized[\"b\"]]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset type: {}\".format(DATASET_TYPE))\n",
    "\n",
    "    print(\"a\", len(a_s))\n",
    "    print(\"b\", len(b_s))\n",
    "    return a_s, b_s, []\n",
    "\n",
    "\n",
    "def get_sentences(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        corpus_data = yaml.safe_load(f)\n",
    "\n",
    "    seeds = corpus_data[\"seeds\"]\n",
    "    distilled = corpus_data[\"distilled\"]\n",
    "    summarized = corpus_data[\"summarized\"]\n",
    "    names = corpus_data[\"names\"]\n",
    "    dataset = corpus_data[\"dataset\"]\n",
    "\n",
    "    a_s = []\n",
    "    b_s = []\n",
    "    for datum in dataset:\n",
    "        a_first = datum[\"a_first\"]\n",
    "        b_first = datum[\"b_first\"]\n",
    "\n",
    "        # Add a sentences\n",
    "        a_s += a_first[\"a\"]\n",
    "        a_s += b_first[\"a\"]\n",
    "\n",
    "        # Add b sentences\n",
    "        b_s += a_first[\"b\"]\n",
    "        b_s += b_first[\"b\"]\n",
    "\n",
    "    print(\"a\", len(a_s))\n",
    "    print(\"b\", len(b_s))\n",
    "\n",
    "    # Shuffle a_s and b_s\n",
    "    random.shuffle(a_s)\n",
    "    random.shuffle(b_s)\n",
    "\n",
    "    # Return\n",
    "    return a_s, b_s, []\n",
    "\n",
    "\n",
    "# def load_experiments(name):\n",
    "#     path = os.path.join(\"narrative_sets\", name)\n",
    "\n",
    "#     with open(os.path.join(path, \"helpful\" + NARRATIVE_SUFFIX + \".txt\"), \"r\") as f:\n",
    "#         helpful = f.read().splitlines()\n",
    "\n",
    "#     with open(os.path.join(path, \"unhelpful\" + NARRATIVE_SUFFIX + \".txt\"), \"r\") as f:\n",
    "#         unhelpful = f.read().splitlines()\n",
    "\n",
    "#     # Neutral set might not exist, if so just use an empty list\n",
    "#     if not os.path.exists(os.path.join(path, \"neutral\" + NARRATIVE_SUFFIX + \".txt\")):\n",
    "#         neutral = []\n",
    "#     else:\n",
    "#         with open(os.path.join(path, \"neutral\" + NARRATIVE_SUFFIX + \".txt\"), \"r\") as f:\n",
    "#             neutral = f.read().splitlines()\n",
    "\n",
    "#     # # Drop texts less than 10 words and more than 50 words.\n",
    "#     # helpful = [text for text in helpful if len(text.split()) >= 10 and len(text.split()) <= 50]\n",
    "#     # unhelpful = [text for text in unhelpful if len(text.split()) >= 10 and len(text.split()) <= 50]\n",
    "#     # neutral = [text for text in neutral if len(text.split()) >= 10 and len(text.split()) <= 50]\n",
    "\n",
    "#     # Drop empty texts\n",
    "#     helpful = [text for text in helpful if len(text) > 0]\n",
    "#     unhelpful = [text for text in unhelpful if len(text) > 0]\n",
    "#     neutral = [text for text in neutral if len(text) > 0]\n",
    "\n",
    "#     return helpful, unhelpful, neutral\n",
    "\n",
    "\n",
    "# print(\"*** Experiment {} ***\".format(NARRATIVE_SET))\n",
    "# helpful, unhelpful, neutral = load_experiments(NARRATIVE_SET)\n",
    "\n",
    "# print(\"\\tHelpful: {}\".format(len(helpful)))\n",
    "# print(\"\\tUnhelpful: {}\".format(len(unhelpful)))\n",
    "# print(\"\\tNeutral: {}\".format(len(neutral)))\n",
    "\n",
    "\n",
    "def align_text_to_conditioners(label, narrative_sets, text, ctx, sample_size):\n",
    "    # No batching\n",
    "    deltas = []\n",
    "    for conditioners in narrative_sets:\n",
    "        conditioner_string = join_many_strings(conditioners)\n",
    "        deltas.append(\n",
    "            {\n",
    "                \"conditioners\": conditioners,\n",
    "                \"likelihood_delta\": likelihood_delta(\n",
    "                    conditioner_string, text[\"text\"], ctx\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return {\"text\": text, \"likelihood_deltas\": deltas}\n",
    "\n",
    "\n",
    "def align(label, narrative_set, texts, sample_size):\n",
    "    print(\"*** Aligning {} texts to {} ***\".format(len(texts), label))\n",
    "    results = []\n",
    "    for text in tqdm(texts):\n",
    "        results.append(\n",
    "            align_text_to_conditioners(\n",
    "                label,\n",
    "                pairs[(tuple(narrative_set), text[\"text\"])],\n",
    "                text,\n",
    "                ctx,\n",
    "                sample_size,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Load texts from corpora/{CORPUS}.json\n",
    "# with open(os.path.join(\"corpora\", f\"{CORPUS}.json\"), \"r\") as f:\n",
    "#     corpus_data = yaml.safe_load(f)\n",
    "from import_data import import_data\n",
    "\n",
    "if CORPUS_NAME in (\"voice\", \"climate\", \"asylum_seekers\"):\n",
    "    X, y, seeds, distilled, summarized, names = import_data(CORPUS)\n",
    "\n",
    "    helpful = [seed[\"a\"] for seed in seeds]\n",
    "    unhelpful = [seed[\"b\"] for seed in seeds]\n",
    "    neutral = []\n",
    "\n",
    "    a_sentences = []\n",
    "    b_sentences = []\n",
    "    for _x, _y in zip(X, y):\n",
    "        if _y == \"a\":\n",
    "            a_sentences.append(_x)\n",
    "        elif _y == \"b\":\n",
    "            b_sentences.append(_x)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid label: {}\".format(_y))\n",
    "\n",
    "    sentences = a_sentences + b_sentences\n",
    "    random.shuffle(sentences)\n",
    "    corpus_data = [\n",
    "        {\"text\": text, \"speakername\": label, \"date\": \"2024-03-06\"}\n",
    "        for text, label in zip(sentences, y)\n",
    "    ]\n",
    "\n",
    "else:\n",
    "    helpful, unhelpful, neutral = get_seeds(CORPUS)\n",
    "    a_sentences, b_sentences, _ = get_sentences(CORPUS)\n",
    "    # sentences = a_sentences + b_sentences\n",
    "    sentences = []\n",
    "    for a in a_sentences:\n",
    "        sentences.append((a, \"a\"))\n",
    "    for b in b_sentences:\n",
    "        sentences.append((b, \"b\"))\n",
    "    random.shuffle(sentences)\n",
    "    corpus_data = [\n",
    "        {\"text\": text, \"speakername\": label, \"date\": \"2024-03-06\"}\n",
    "        for text, label in sentences\n",
    "    ]\n",
    "\n",
    "# # Debug, get only 10 sentences\n",
    "# corpus_data = corpus_data[:10]\n",
    "\n",
    "print(\"Loaded {} texts from corpus\".format(len(corpus_data)))\n",
    "print(\"Total word count:\", sum([len(text[\"text\"].split()) for text in corpus_data]))\n",
    "\n",
    "# Use SAMPLE_COUNT\n",
    "if SAMPLE_COUNT != -1 and len(corpus_data) > SAMPLE_COUNT:\n",
    "    corpus_data = random.sample(corpus_data, SAMPLE_COUNT)\n",
    "\n",
    "\n",
    "# Precompute narrative set -- text pairs\n",
    "pairs = {}\n",
    "to_precompute = set()\n",
    "for narrative_set in [helpful, unhelpful, neutral]:\n",
    "    for text in corpus_data:\n",
    "        # # Pick a random narrative set\n",
    "        # narrative_sets = list(itertools.permutations(narrative_set, CONDITIONERS))\n",
    "        # narrative_sets = random.sample(narrative_sets, min(len(narrative_sets), len(narrative_set)))\n",
    "        narrative_sets = []\n",
    "        for _ in range(SAMPLE_MULTIPLIER):\n",
    "            narrative_sets_iter = stratified_n_tuple_sampling(\n",
    "                list(narrative_set), CONDITIONERS\n",
    "            )\n",
    "            narrative_sets.extend(narrative_sets_iter)\n",
    "\n",
    "        # Append to the list\n",
    "        pairs[(tuple(narrative_set), text[\"text\"])] = narrative_sets\n",
    "\n",
    "        # Add to the set of things to precompute\n",
    "        for conditioners in narrative_sets:\n",
    "            conditioner_string = join_many_strings(conditioners)\n",
    "            to_precompute.add(join_many_strings([conditioner_string, text[\"text\"]]))\n",
    "            to_precompute.add(conditioner_string)\n",
    "            to_precompute.add(text[\"text\"])\n",
    "\n",
    "# Get the total word count of the precompute set\n",
    "print(\n",
    "    \"Total word count of precompute set:\",\n",
    "    sum([len(text.split()) for text in to_precompute]),\n",
    ")\n",
    "# break\n",
    "\n",
    "# Precompute the logits\n",
    "print(\"--- Precomputing logits ---\")\n",
    "logits, usages = get_logits(list(to_precompute), ctx.model)\n",
    "print(usages[0])\n",
    "print(\"--- Done precomputing logits ---\")\n",
    "\n",
    "# Run experiments\n",
    "results = {\n",
    "    \"helpful\": align(\"helpful\", helpful, corpus_data, None),\n",
    "    \"unhelpful\": align(\"unhelpful\", unhelpful, corpus_data, None),\n",
    "    # \"neutral\": align(\"neutral\", neutral, corpus_data, SAMPLE_SIZE)\n",
    "}\n",
    "\n",
    "# Save results with pickle\n",
    "with open(\n",
    "    os.path.join(\n",
    "        \"corpus_results/diff_classification\",\n",
    "        EXPERIMENT_NAME,\n",
    "        # f\"{CORPUS}_{MODEL}_{CONDITIONERS}_{SAMPLE_MULTIPLIER}.pkl\",\n",
    "        \"results.pkl\",\n",
    "    ),\n",
    "    \"wb\",\n",
    ") as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc296c7b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save usage data to CSV\n",
    "# Usage object is like\n",
    "# CompletionUsage(completion_tokens=1, prompt_tokens=43, total_tokens=44)\n",
    "# We want to save this as two columns - prompt_tokens and completion_tokens\n",
    "import pandas as pd\n",
    "\n",
    "usage_data = []\n",
    "for usage in usages:\n",
    "    usage_data.append(\n",
    "        {\n",
    "            \"prompt_tokens\": usage.prompt_tokens,\n",
    "            \"completion_tokens\": usage.completion_tokens,\n",
    "        }\n",
    "    )\n",
    "\n",
    "usage_df = pd.DataFrame(usage_data)\n",
    "usage_df.to_csv(\n",
    "    os.path.join(\n",
    "        \"corpus_results/diff_classification\",\n",
    "        EXPERIMENT_NAME,\n",
    "        # f\"{CORPUS}_{MODEL}_{CONDITIONERS}_{SAMPLE_MULTIPLIER}_usage.csv\",\n",
    "        \"usage.csv\",\n",
    "    ),\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e428964c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "\n",
    "# Load data from pkl\n",
    "with open(\n",
    "    os.path.join(\n",
    "        \"corpus_results/diff_classification\",\n",
    "        EXPERIMENT_NAME,\n",
    "        # f\"{CORPUS}_{MODEL}_{CONDITIONERS}_{SAMPLE_MULTIPLIER}.pkl\",\n",
    "        \"results.pkl\",\n",
    "    ),\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# For each narrative set in (helpful, unhelpful), find the texts with the highest and lowest likelihood delta (relative to the other set\n",
    "# of narratives) and print them.\n",
    "helpful = results[\"helpful\"]\n",
    "unhelpful = results[\"unhelpful\"]\n",
    "\n",
    "\n",
    "def compute_deltas(result_set):\n",
    "    # Compute generally for helpful/unhelpful/neutral\n",
    "    deltas = {}\n",
    "    for text in result_set:\n",
    "        text_deltas = [\n",
    "            delta[\"likelihood_delta\"][\"likelihood_delta\"]\n",
    "            for delta in text[\"likelihood_deltas\"]\n",
    "        ]\n",
    "        deltas[text[\"text\"][\"text\"]] = np.mean(text_deltas)\n",
    "        # Get the signed max abs value\n",
    "        # deltas[text[\"text\"][\"text\"]] = max(text_deltas, key=abs)\n",
    "        # deltas[text[\"text\"][\"text\"]] = max(text_deltas)\n",
    "        print(text)\n",
    "    return deltas\n",
    "\n",
    "\n",
    "helpful_deltas = compute_deltas(helpful)\n",
    "unhelpful_deltas = compute_deltas(unhelpful)\n",
    "\n",
    "helpful_unhelpful_diffs = []\n",
    "for text in helpful:\n",
    "    helpful_unhelpful_diffs.append(\n",
    "        {\n",
    "            \"text\": text[\"text\"][\"text\"],\n",
    "            \"helpful\": helpful_deltas[text[\"text\"][\"text\"]],\n",
    "            \"unhelpful\": unhelpful_deltas[text[\"text\"][\"text\"]],\n",
    "            # \"helpful_deltas\": [delta[\"likelihood_delta\"][\"likelihood_delta\"] for delta in text[\"likelihood_deltas\"]],\n",
    "            # \"unhelpful_deltas\": [delta[\"likelihood_delta\"][\"likelihood_delta\"] for delta in text[\"likelihood_deltas\"]],\n",
    "            \"speaker\": text[\"text\"][\"speakername\"],\n",
    "            \"affiliation\": (\n",
    "                text[\"text\"][\"affiliation\"] if \"affiliation\" in text[\"text\"] else None\n",
    "            ),\n",
    "            \"date\": text[\"text\"][\"date\"],\n",
    "            \"diff\": helpful_deltas[text[\"text\"][\"text\"]]\n",
    "            - unhelpful_deltas[text[\"text\"][\"text\"]],\n",
    "        }\n",
    "    )\n",
    "\n",
    "helpful_unhelpful_diffs = sorted(helpful_unhelpful_diffs, key=lambda x: x[\"diff\"])\n",
    "\n",
    "print(\"Most unhelpful:\")\n",
    "for text in helpful_unhelpful_diffs[:10]:\n",
    "    print(f\"[{text['diff']}] {text['text']}\")\n",
    "print()\n",
    "print(\"Most helpful:\")\n",
    "for text in helpful_unhelpful_diffs[-10:]:\n",
    "    print(f\"[{text['diff']}] {text['text']}\")\n",
    "print()\n",
    "\n",
    "outpath = os.path.join(\n",
    "    \"corpus_results/diff_classification\",\n",
    "    EXPERIMENT_NAME,\n",
    "    # f\"{CORPUS}_{MODEL}_{CONDITIONERS}_{SAMPLE_MULTIPLIER}\",\n",
    "    \"results\"\n",
    ")\n",
    "print(\"Saving to:\", outpath)\n",
    "\n",
    "# Save results with JSON\n",
    "with open(outpath + \".json\", \"w\") as f:\n",
    "    json.dump(helpful_unhelpful_diffs, f, indent=4)\n",
    "\n",
    "# Save in CSV format\n",
    "df = pd.DataFrame(helpful_unhelpful_diffs)\n",
    "df.to_csv(outpath + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e4bb9b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1.823423,
   "end_time": "2024-05-17T11:16:53.263303",
   "environment_variables": {},
   "exception": true,
   "input_path": "new_method_experiment.ipynb",
   "output_path": "new_method_experiment_asylum_seekers.ipynb",
   "parameters": {},
   "start_time": "2024-05-17T11:16:51.439880",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}